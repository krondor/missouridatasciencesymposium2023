{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "Insert Project Token Below by clicking the Ellipsis points in the menu bar and clicking `insert project token`.\n\n**Note:  Must have created a project token prior to in project options.**"}, {"metadata": {}, "cell_type": "code", "source": "# insert project token", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n# Use Watsonx to respond to natural language questions using RAG approach"}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "This notebook contains the steps and code to demonstrate support of Retrieval Augumented Generation in watsonx.ai. It introduces commands for data retrieval, knowledge base building & querying, and model testing.\n\nSome familiarity with Python is helpful. This notebook uses Python 3.10.\n\n#### About Retrieval Augmented Generation\nRetrieval Augmented Generation (RAG) is a versatile pattern that can unlock a number of use cases requiring factual recall of information, such as querying a knowledge base in natural language.\n\nIn its simplest form, RAG requires 3 steps:\n\n- Index knowledge base passages (once)\n- Retrieve relevant passage(s) from knowledge base (for every user query)\n- Generate a response by feeding retrieved passage into a large language model (for every user query)\n"}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "<a id=\"setup\"></a>\n##  Set up the environment\n\n"}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Install and import dependecies"}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "!pip install chromadb==0.3.27 | tail -n 1\n!pip install sentence_transformers | tail -n 1\n!pip install pandas | tail -n 1\n!pip install rouge_score | tail -n 1\n!pip install nltk | tail -n 1\n!pip install \"ibm-watson-machine-learning>=1.0.312\" | tail -n 1", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**Note:** Please restart the notebook kernel to pick up proper version of packages installed above."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "import os, getpass\nimport pandas as pd\nfrom typing import Optional, Dict, Any, Iterable, List\n\ntry:\n    from sentence_transformers import SentenceTransformer\nexcept ImportError:\n    raise ImportError(\"Could not import sentence_transformers: Please install sentence-transformers package.\")\n    \ntry:\n    import chromadb\n    from chromadb.api.types import EmbeddingFunction\nexcept ImportError:\n    raise ImportError(\"Could not import chromdb: Please install chromadb package.\")", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Watsonx API connection\nThis cell defines the credentials required to work with watsonx API for Foundation\nModel inferencing.\n\n**Action:** Provide the IBM Cloud user API key. For details, see\n[documentation](https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui)."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "credentials = {\n    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n    \"apikey\": getpass.getpass(\"Please enter your WML api key (hit enter): \")\n}", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Defining the project id\nThe API requires project id that provides the context for the call. We will obtain the id from the project in which this notebook runs. Otherwise, please provide the project id.\n\n**Hint**: You can find the `project_id` as follows. Open the prompt lab in watsonx.ai. At the very top of the UI, there will be `Projects / <project name> /`. Click on the `<project name>` link. Then get the `project_id` from Project's Manage tab (Project -> Manage -> General -> Details).\n"}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "try:\n    project_id = os.environ[\"PROJECT_ID\"]\nexcept KeyError:\n    project_id = input(\"Please enter your project_id (hit enter): \")", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "<a id=\"data\"></a>\n## Train/test data loading"}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "Load train and test datasets. At first, training dataset (`train_data`) should be used to work with the models to prepare and tune prompt. Then, test dataset (`test_data`) should be used to calculate the metrics score for selected model, defined prompts and parameters."}, {"metadata": {}, "cell_type": "code", "source": "!mkdir -p data/RAG/nq910_400_instances", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Download the script to the file system of your runtime\nwslib.download_file(\"test.tsv\")\nwslib.download_file(\"train.tsv\")\n!mv *.tsv data/RAG/nq910_400_instances/", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "filename_test = 'data/RAG/nq910_400_instances/test.tsv'\nfilename_train = 'data/RAG/nq910_400_instances/train.tsv'\n\ntest_data = pd.read_csv(filename_test, delimiter='\\t')\ntrain_data = pd.read_csv(filename_train, delimiter='\\t')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "train_data.head()", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "test_data.head()", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "## Build up knowledge base\n\nThe current state-of-the-art in RAG is to create dense vector representations of the knowledge base in order to calculate the semantic similarity to a given user query.\n\nWe can generate dense vector representations using embedding models. In this notebook, we use [SentenceTransformers](https://www.google.com/search?client=safari&rls=en&q=sentencetransformers&ie=UTF-8&oe=UTF-8) [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) to embed both the knowledge base passages and user queries. `all-MiniLM-L6-v2` is a performant open-source model that is small enough to run locally.\n\nA vector database is optimized for dense vector indexing and retrieval. This notebook uses [Chroma](https://docs.trychroma.com), a user-friendly open-source vector database, licensed under Apache 2.0, which offers good speed and performance with all-MiniLM-L6-v2 embedding model."}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "The dataset we are using is already split into self-contained passages that can be ingested by Chroma. \n\nThe size of each passage is limited by the embedding model's context window (which is 256 tokens for `all-MiniLM-L6-v2`)."}, {"metadata": {}, "cell_type": "markdown", "source": "### Load knowledge base documents\n\nLoad set of documents used further to build knowledge base. "}, {"metadata": {}, "cell_type": "code", "source": "data_root = \"data\"\nknowledge_base_dir = f\"{data_root}/knowledge_base\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Fetch knowledge_base\nPull `knowledge_base.zip` from box.  Comment out `wget` and `wslib.save()` below and uncomment `wslib` if already present in the project.  "}, {"metadata": {}, "cell_type": "code", "source": "!curl -L  https://ibm.box.com/shared/static/b8ibw5m1504q2it5ciw833hhk7qhzu8g.zipz --output knowledge_base.zip", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#wslib.download_file(\"knowledge_base.zip\")\n!mv knowledge_base.zip data/", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "wslib.upload_file(\"data/knowledge_base.zip\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "if not os.path.exists(knowledge_base_dir):\n    from zipfile import ZipFile\n    with ZipFile(knowledge_base_dir + \".zip\", 'r') as zObject:\n        zObject.extractall(data_root)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "documents = pd.read_csv(f\"{knowledge_base_dir}/psgs.tsv\", sep='\\t', header=0)\ndocuments['indextext'] = documents['title'].astype(str) + \"\\n\" + documents['text']", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Create an embedding function\n\nNote that you can feed a custom embedding function to be used by chromadb. The performance of chromadb may differ depending on the embedding model used."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "class MiniLML6V2EmbeddingFunction(EmbeddingFunction):\n    MODEL = SentenceTransformer('all-MiniLM-L6-v2')\n    def __call__(self, texts):\n        return MiniLML6V2EmbeddingFunction.MODEL.encode(texts).tolist()\nemb_func = MiniLML6V2EmbeddingFunction()", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Set up Chroma upsert\n\nUpserting a document means update the document even if it exists in the database. Otherwise re-inserting a document throws an error. This is useful for experimentation purpose."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "class ChromaWithUpsert:\n    def __init__(\n            self,\n            name: Optional[str] = \"watsonx_rag_collection\",\n            persist_directory:Optional[str]=None,\n            embedding_function: Optional[EmbeddingFunction]=None,\n            collection_metadata: Optional[Dict] = None,\n    ):\n        self._client_settings = chromadb.config.Settings()\n        if persist_directory is not None:\n            self._client_settings = chromadb.config.Settings(\n                chroma_db_impl=\"duckdb+parquet\",\n                persist_directory=persist_directory,\n            )\n        self._client = chromadb.Client(self._client_settings)\n        self._embedding_function = embedding_function\n        self._persist_directory = persist_directory\n        self._name = name\n        self._collection = self._client.get_or_create_collection(\n            name=self._name,\n            embedding_function=self._embedding_function\n            if self._embedding_function is not None\n            else None,\n            metadata=collection_metadata,\n        )\n\n    def upsert_texts(\n        self,\n        texts: Iterable[str],\n        metadata: Optional[List[dict]] = None,\n        ids: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> List[str]:\n        \"\"\"Run more texts through the embeddings and add to the vectorstore.\n        Args:\n            :param texts (Iterable[str]): Texts to add to the vectorstore.\n            :param metadatas (Optional[List[dict]], optional): Optional list of metadatas.\n            :param ids (Optional[List[str]], optional): Optional list of IDs.\n            :param metadata: Optional[List[dict]] - optional metadata (such as title, etc.)\n        Returns:\n            List[str]: List of IDs of the added texts.\n        \"\"\"\n        # TODO: Handle the case where the user doesn't provide ids on the Collection\n        if ids is None:\n            import uuid\n            ids = [str(uuid.uuid1()) for _ in texts]\n        embeddings = None\n        self._collection.upsert(\n            metadatas=metadata, documents=texts, ids=ids\n        )\n        return ids\n\n    def is_empty(self):\n        return self._collection.count()==0\n\n    def persist(self):\n        self._client.persist()\n\n    def query(self, query_texts:str, n_results:int=5):\n        \"\"\"\n        Returns the closests vector to the question vector\n        :param query_texts: the question\n        :param n_results: number of results to generate\n        :return: the closest result to the given question\n        \"\"\"\n        return self._collection.query(query_texts=query_texts, n_results=n_results)", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Embed and index documents with Chroma\n\n**Note: Could take several minutes if you don't have pre-built indices**"}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "%%time\nchroma = ChromaWithUpsert(\n    name=f\"nq910_minilm6v2\",\n    embedding_function=emb_func,  # you can have something here using /embed endpoint\n    persist_directory=knowledge_base_dir,\n)\nif chroma.is_empty():\n    _ = chroma.upsert_texts(\n        texts=documents.indextext.tolist(),\n        # we handle tokenization, embedding, and indexing automatically. You can skip that and add your own embeddings as well\n        metadata=[{'title': title, 'id': id}\n                  for (title,id) in\n                  zip(documents.title, documents.id)],  # filter on these!\n        ids=[str(i) for i in documents.id],  # unique for each doc\n    )\n    chroma.persist()", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "<a id=\"models\"></a>\n## Foundation Models on Watsonx"}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "You need to specify `model_id` that will be used for inferencing."}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "**Action**: Use `FLAN_UL2` model."}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "model_id = ModelTypes.FLAN_UL2", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "<a id=\"predict\"></a>\n## Generate a retrieval-augmented response to a question"}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Select questions\n\nGet questions from the previously loaded test dataset."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "question_texts = [q.strip(\"?\") + \"?\" for q in test_data['question'].tolist()]\nprint(\"\\n\".join(question_texts))", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Retrieve relevant context\n\nFetch paragraphs similar to the question."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "relevant_contexts = []\n\nfor question_text in question_texts:\n    relevant_chunks = chroma.query(\n        query_texts=[question_text],\n        n_results=5,\n    )\n    relevant_contexts.append(relevant_chunks)", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "Get the set of chunks for one of the questions."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "sample_chunks = relevant_contexts[0]\nfor i, chunk in enumerate(sample_chunks['documents'][0]):\n    print(\"=========\")\n    print(\"Paragraph index : \", sample_chunks['ids'][0][i])\n    print(\"Paragraph : \", chunk)\n    print(\"Distance : \", sample_chunks['distances'][0][i])", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Feed the context and the questions to `watsonx.ai` model."}, {"metadata": {}, "cell_type": "markdown", "source": "Define instructions for the model.\n\n**Note:** Please start with finding better prompts using small subset of training records (under `train_data` variable). Make sure to not run an inference of all of `train_data`, as it'll take a long time to get the results. To get a sample from `train_data`, you can use e.g.`train_data.head(n=10)` to get first 10 records, or `train_data.sample(n=10)` to get random 10 records. Only once you have identified the best performing prompt, update this notebook to use the prompt and compute the metrics on the test data.\n\n**Action:** Please edit the below cell and add your own prompt here. In the below prompt, we have the instruction (first sentence) and one example included in the prompt. If you want to change the prompt or add your own examples or more examples, please change the below prompt accordingly."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "def make_prompt(context, question_text):\n    return (f\"You are a question answering chatbot. I will ask you to answer a question, \" \\\n            \"and you will come up with an answer based on the context provided. \" \\\n            \"Please say Don't Know if you do not know the answer. \" \\\n            \"However, if you do know the answer, please follow the following guidelines: \" \\\n            \"Always use a person's full name (including middle name), \" \\\n            \"do not respond to a question with another question, \" \\\n            \"and be very specific by using 'a' or 'the' instead of just the name \" \\\n            \"(for example say 'the denver broncos' instead of 'denver broncos').\\n\"\n          + f\"{context}:\\n\\n\"\n          + f\"{question_text}\")\n\nprompt_texts = []\n\nfor relevant_context, question_text in zip(relevant_contexts, question_texts):\n    context = \"\\n\\n\\n\".join(relevant_context[\"documents\"][0])\n    prompt_text = make_prompt(context, question_text)\n    prompt_texts.append(prompt_text)", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "Inspect prompt for sample question."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "print(prompt_texts[0])", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Defining the model parameters\nWe need to provide a set of model parameters that will influence the result:"}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watson_machine_learning.foundation_models.utils.enums import DecodingMethods\n\nparameters = {\n    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n    GenParams.MIN_NEW_TOKENS: 2,\n    GenParams.MAX_NEW_TOKENS: 50\n}", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "Initialize the `Model` class."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "from ibm_watson_machine_learning.foundation_models import Model\n\nmodel = Model(\n    model_id=model_id,\n    params=parameters,\n    credentials=credentials,\n    project_id=project_id)", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Generate a retrieval-augmented response"}, {"metadata": {}, "cell_type": "markdown", "source": "**Note:** Execution of this cell could take several minutes."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "%%time\nresults = []\n\nfor prompt_text in prompt_texts:\n    results.append(model.generate_text(prompt=prompt_text))", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "for idx, result in enumerate(results):\n    print(\"Question = \", test_data.iloc[idx]['question'])\n    print(\"Answer = \", result)\n    print(\"Expected Answer(s) (may not be appear with exact wording in the dataset) = \", test_data.iloc[idx]['answers'])\n    print(\"\\n\")", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "<a id=\"score\"></a>\n## Calculate rougeL metric"}, {"metadata": {}, "cell_type": "markdown", "source": "In this sample notebook `rouge_score` module was used for rougeL calculation."}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "#### Rouge Metric"}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "**Note:** The Rouge (Recall-Oriented Understudy for Gisting Evaluation) metric is a set of evaluation measures used in natural language processing (NLP) and specifically in text summarization and machine translation tasks. The Rouge metrics are designed to assess the quality of generated summaries or translations by comparing them to one or more reference texts.\n\nThe main idea behind Rouge is to measure the overlap between the generated summary (or translation) and the reference text(s) in terms of n-grams or longest common subsequences. By calculating recall, precision, and F1 scores based on these overlapping units, Rouge provides a quantitative assessment of the summary's content overlap with the reference(s).\n\nRouge-1 focuses on individual word overlap, Rouge-2 considers pairs of consecutive words, and Rouge-L takes into account the ordering of words and phrases. These metrics provide different perspectives on the similarity between two texts and can be used to evaluate different aspects of summarization or text generation models."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "from rouge_score import rouge_scorer\nfrom collections import defaultdict\nimport numpy as np\n\ndef get_rouge_score(predictions, references):\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'])\n    aggregate_score = defaultdict(list)\n\n    for result, ref in zip(predictions, references):\n        for key, val in scorer.score(result, ref).items():\n            aggregate_score[key].append(val.fmeasure)\n\n    scores = {}\n    for key in aggregate_score:\n        scores[key] = np.mean(aggregate_score[key])\n    \n    return scores", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(get_rouge_score(results, test_data.answers))", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "---"}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "Copyright \u00a9 2023 IBM. This notebook and its source code are released under the terms of the MIT License."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 4}